{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c9d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from escriptorium_connector import EscriptoriumConnector\n",
    "from escriptorium_connector.dtos import PostProject, PostDocument, ReadDirection, LineOffset\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import io \n",
    "from io import BytesIO\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec00594",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EscriptoriumConnector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m PROJECT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGnazim Data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize the connector using the API token\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m connector \u001b[38;5;241m=\u001b[39m \u001b[43mEscriptoriumConnector\u001b[49m(ESCIPTORIUM_URL, USERNAME, PASSWORD)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#new_project = connector.get_project(connector.get_project_pk_by_name(PROJECT_NAME))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# project_slug = connector.get_project(connector.get_project_pk_by_name(PROJECT_NAME)).slug\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EscriptoriumConnector' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# eScriptorium instance details\n",
    "ESCIPTORIUM_URL = 'https://env-9828609.us.reclaim.cloud/'#'http://localhost:8888' \n",
    "API_TOKEN = 'c85e1c7e3d081ddd86ee70c58f2d3c66d6e9c6da'#'02835aac5aee131047198e5637b02145ae3e828e'  \n",
    "USERNAME = 'admin'\n",
    "PASSWORD = '84aÂ£92oGhC'#'admin'\n",
    "PROJECT_NAME = 'Gnazim Data'\n",
    "\n",
    "# Initialize the connector using the API token\n",
    "connector = EscriptoriumConnector(ESCIPTORIUM_URL, USERNAME, PASSWORD)\n",
    "#new_project = connector.get_project(connector.get_project_pk_by_name(PROJECT_NAME))\n",
    "# project_slug = connector.get_project(connector.get_project_pk_by_name(PROJECT_NAME)).slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629a3f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the xml from escriptorium\n",
    "bytes_file = connector.download_part_alto_transcription(document_pk=50, part_pk=3143, transcription_pk=94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f46dfdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved IDGNAZIM0001008.xml to XML Exports\\IDGNAZIM0001008.xml\n",
      "Saved METS.xml to XML Exports\\METS.xml\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "def unzip_and_save_xml(zip_bytes, output_folder):\n",
    "    \"\"\"\n",
    "    Unzips the provided bytes (ZIP format) and saves each XML file in the specified folder.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Use an in-memory file for the zip bytes\n",
    "    with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zf:\n",
    "        # Loop over each file in the ZIP\n",
    "        for file_name in zf.namelist():\n",
    "            # Check if it's an XML file\n",
    "            if file_name.endswith('.xml'):\n",
    "                with zf.open(file_name) as xml_file:\n",
    "                    # Read XML file contents\n",
    "                    xml_content = xml_file.read().decode('utf-8')\n",
    "                    \n",
    "                    # Define full path for the output XML file\n",
    "                    output_path = os.path.join(output_folder, file_name)\n",
    "                    \n",
    "                    # Write the XML content to a file\n",
    "                    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(xml_content)\n",
    "                    print(f\"Saved {file_name} to {output_path}\")\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "output_folder = \"XML Exports\"\n",
    "unzip_and_save_xml(bytes_file, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa8f390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as Image Exports\\IDGNAZIM0001008.tif\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "def save_image_from_bytes(image_bytes, output_folder, image_name=\"output_image.png\"):\n",
    "    \"\"\"\n",
    "    Converts image bytes to an image and saves it in the specified folder.\n",
    "    \"\"\"\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Convert the bytes to an image\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    \n",
    "    # Define full path for the output image file\n",
    "    output_path = os.path.join(output_folder, image_name)\n",
    "    \n",
    "    # Save the image (can specify the format here, e.g., PNG, JPEG)\n",
    "    image.save(output_path)\n",
    "    print(f\"Image saved as {output_path}\")\n",
    "\n",
    "# Example usage:\n",
    "output_folder = \"Image Exports\"\n",
    "save_image_from_bytes(image_bytes, output_folder, \"IDGNAZIM0001008.tif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a62906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bytes = connector.get_document_part_image(doc_pk=50, part_pk=3143,)#.download_part_alto_transcription(document_pk=50, part_pk=3143, transcription_pk=94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "324caf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new project: Persian Model Training\n"
     ]
    }
   ],
   "source": [
    "# create new project\n",
    "\n",
    "# Create a new project\n",
    "new_project_name = 'persian-model-training'  # Give your project a name\n",
    "project_slug = new_project_name\n",
    "project_data = PostProject(name=PROJECT_NAME, slug=new_project_name)  # The slug is usually a URL-friendly version of the name\n",
    "new_project = connector.create_project(project_data)\n",
    "\n",
    "print(f\"Created new project: {new_project.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f5b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont forget to check whther you need to clean xmls from their \"word\" elements, and if so, use the XML editor notebook\n",
    "\n",
    "#functions\n",
    "def upload_images_to_document(connector, folder_path, document_id):\n",
    "    \"\"\"Uploads image files to a specified document in eScriptorium.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Add/remove extensions as needed\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                # Prepare image data for upload\n",
    "                image_data = file.read()\n",
    "\n",
    "                # Create an instance of the dataclass\n",
    "                image_data_info = ImageDataInfo(filename=filename)\n",
    "\n",
    "                # Upload the image\n",
    "                #print(f\"Uploading {filename} to document {document_id}...\")\n",
    "                response = connector.create_document_part(\n",
    "                    document_pk=document_id, \n",
    "                    image_data_info=image_data_info, \n",
    "                    filename=filename, \n",
    "                    image_data=image_data\n",
    "                )\n",
    "                print(f\"Finished uploading {filename}\")\n",
    "\n",
    "@dataclass\n",
    "class ImageDataInfo:\n",
    "    filename: str\n",
    "    # Add other fields here if needed                \n",
    "                \n",
    "def upload_xmls_to_document(connector, folder_path, document_id, transcription_name):\n",
    "    \"\"\"Uploads image files to a specified document in eScriptorium.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.xml'):  # Add/remove extensions as needed\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                # Prepare image data for upload\n",
    "                file_data = file.read()\n",
    "\n",
    "                # Upload the image\n",
    "                print(f\"Uploading {filename} to document {document_id}...\")\n",
    "                response = connector.upload_part_transcription(\n",
    "                    document_pk=document_id, \n",
    "                    transcription_name = transcription_name, \n",
    "                    filename=filename, \n",
    "                    file_data=file_data,\n",
    "                    override='off' #change to on if you want to override existing transciptions\n",
    "                )\n",
    "                print(f\"Finished uploading {filename}\")\n",
    "\n",
    "def iterate_documents(root_folder, connector):\n",
    "    \"\"\"\n",
    "    Iterates through each subfolder in the root_folder. Each subfolder is considered a document.\n",
    "\n",
    "    :param root_folder: The path to the root folder containing document subfolders.\n",
    "    \"\"\"\n",
    "    # Iterate through each item in the root folder\n",
    "    for item in os.listdir(root_folder):\n",
    "        # Construct the full path of the item\n",
    "        item_path = os.path.join(root_folder, item)\n",
    "        \n",
    "        # Check if the item is a directory (i.e., a document folder)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"Document Name: {item}\")\n",
    "            main_script = \"Hebrew\"\n",
    "            document_data = PostDocument(\n",
    "                name=item,\n",
    "                project=project_slug,\n",
    "                main_script=main_script,\n",
    "                read_direction=ReadDirection.RTL,\n",
    "                line_offset=LineOffset.BASELINE,\n",
    "                tags=[]\n",
    "            )\n",
    "            #create document\n",
    "            document_pk = create_document_debug(connector, document_data)\n",
    "            #upload images\n",
    "            image_folder = item_path  # Assuming images are directly in the document folder\n",
    "            upload_images_to_document(connector, image_folder, document_pk)\n",
    "            #upload page xmls\n",
    "            transcription_name = f'transcript_{datetime.date.today()}' \n",
    "            page_folder = os.path.join(item_path, \"page\")  # The folder containing XML files\n",
    "            upload_xmls_to_document(connector, page_folder, document_pk, transcription_name)\n",
    "            \n",
    "# Modify the create_document method in EscriptoriumConnector class\n",
    "def create_document_debug(connector, doc_data):\n",
    "    url = f\"{connector.api_url}documents/\"\n",
    "    # Convert the dataclass to a dictionary\n",
    "    payload = doc_data.__dict__\n",
    "    print(f\"Request URL: {url}\")\n",
    "    print(f\"Payload: {payload}\")\n",
    "\n",
    "    # Perform the POST request\n",
    "    response = connector.http.post(url, json=payload)\n",
    "\n",
    "    # Print raw response text\n",
    "    print(f\"Raw Response: {response.text}\")\n",
    "    txt = response.text\n",
    "    doc_pk = txt[txt.find('\":')+2:txt.find(',')]\n",
    "    return doc_pk \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"C:/Users/User/PycharmProjects/PersianChildhood/Docs to Upload/FarsiPress\" # Update this to your actual folder path\n",
    "\n",
    "#first initialize an instance from an existing project\n",
    "# connector = EscriptoriumConnector(ESCIPTORIUM_URL, USERNAME, PASSWORD)\n",
    "# new_project = connector.get_project(connector.get_project_pk_by_name(PROJECT_NAME))\n",
    "# project_slug = connector.get_project(connector.get_project_pk_by_name(PROJECT_NAME)).slug\n",
    "\n",
    "iterate_documents(root_folder, connector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8737c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8288d950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gnazim-trial'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize an instance from an existing project\n",
    "new_project = connector.get_project(connector.get_project_pk_by_name(PROJECT_NAME))\n",
    "new_project.slug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97262f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request URL: http://localhost:8080/api/documents/\n",
      "Payload: {'name': 'Gnazim Cards Trial I', 'project': 'gnazim-trial', 'main_script': 'Hebrew', 'read_direction': <ReadDirection.RTL: 'rtl'>, 'line_offset': <LineOffset.BASELINE: 0>, 'tags': [], '__pydantic_initialised__': True}\n",
      "Raw Response: {\"pk\":49,\"name\":\"Gnazim Cards Trial I\",\"project\":\"gnazim-trial\",\"transcriptions\":[{\"pk\":90,\"name\":\"manual\",\"archived\":false,\"avg_confidence\":null}],\"main_script\":\"Hebrew\",\"read_direction\":\"rtl\",\"line_offset\":0,\"show_confidence_viz\":false,\"valid_block_types\":[{\"pk\":1,\"name\":\"Title\"},{\"pk\":2,\"name\":\"Main\"},{\"pk\":3,\"name\":\"Commentary\"},{\"pk\":4,\"name\":\"Illustration\"}],\"valid_line_types\":[],\"valid_part_types\":[],\"tags\":[],\"created_at\":\"2024-04-01T18:25:01.147403Z\",\"updated_at\":\"2024-04-01T18:25:01.147419Z\"}\n"
     ]
    }
   ],
   "source": [
    "#create a new document\n",
    "\n",
    "new_document_name = \"Gnazim Cards Trial I\"\n",
    "main_script = \"Hebrew\"\n",
    "\n",
    "\n",
    "document_data = PostDocument(\n",
    "    name=new_document_name,\n",
    "    project=new_project.slug,\n",
    "    main_script=main_script,\n",
    "    read_direction=ReadDirection.RTL,\n",
    "    line_offset=LineOffset.BASELINE,\n",
    "    tags=[]\n",
    ")\n",
    "\n",
    "# Modify the create_document method in EscriptoriumConnector class\n",
    "def create_document_debug(connector, doc_data):\n",
    "    url = f\"{connector.api_url}documents/\"\n",
    "    # Convert the dataclass to a dictionary\n",
    "    payload = doc_data.__dict__\n",
    "    print(f\"Request URL: {url}\")\n",
    "    print(f\"Payload: {payload}\")\n",
    "\n",
    "    # Perform the POST request\n",
    "    response = connector.http.post(url, json=payload)\n",
    "\n",
    "    # Print raw response text\n",
    "    print(f\"Raw Response: {response.text}\")\n",
    "    txt = response.text\n",
    "    doc_pk = txt[txt.find('\":')+2:txt.find(',')]\n",
    "    return doc_pk \n",
    "\n",
    "# Call the modified create_document method\n",
    "document_pk = create_document_debug(connector, document_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17398254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'49'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a75685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished uploading IDGNAZIM0001.tif\n",
      "Finished uploading IDGNAZIM00010.tif\n",
      "Finished uploading IDGNAZIM000100.tif\n",
      "Finished uploading IDGNAZIM000101.tif\n",
      "Finished uploading IDGNAZIM000102.tif\n",
      "Finished uploading IDGNAZIM000103.tif\n",
      "Finished uploading IDGNAZIM000104.tif\n",
      "Finished uploading IDGNAZIM000105.tif\n",
      "Finished uploading IDGNAZIM000106.tif\n",
      "Finished uploading IDGNAZIM000107.tif\n",
      "Finished uploading IDGNAZIM000108.tif\n",
      "Finished uploading IDGNAZIM000109.tif\n",
      "Finished uploading IDGNAZIM00011.tif\n",
      "Finished uploading IDGNAZIM000110.tif\n",
      "Finished uploading IDGNAZIM000111.tif\n",
      "Finished uploading IDGNAZIM000112.tif\n",
      "Finished uploading IDGNAZIM000113.tif\n",
      "Finished uploading IDGNAZIM000114.tif\n",
      "Finished uploading IDGNAZIM000115.tif\n",
      "Finished uploading IDGNAZIM000116.tif\n",
      "Finished uploading IDGNAZIM000117.tif\n",
      "Finished uploading IDGNAZIM000118.tif\n",
      "Finished uploading IDGNAZIM000119.tif\n",
      "Finished uploading IDGNAZIM00012.tif\n",
      "Finished uploading IDGNAZIM000120.tif\n",
      "Finished uploading IDGNAZIM000121.tif\n",
      "Finished uploading IDGNAZIM000122.tif\n",
      "Finished uploading IDGNAZIM000123.tif\n",
      "Finished uploading IDGNAZIM000124.tif\n",
      "Finished uploading IDGNAZIM000125.tif\n",
      "Finished uploading IDGNAZIM000126.tif\n",
      "Finished uploading IDGNAZIM000127.tif\n",
      "Finished uploading IDGNAZIM000128.tif\n",
      "Finished uploading IDGNAZIM000129.tif\n",
      "Finished uploading IDGNAZIM00013.tif\n",
      "Finished uploading IDGNAZIM000130.tif\n",
      "Finished uploading IDGNAZIM000131.tif\n",
      "Finished uploading IDGNAZIM000132.tif\n",
      "Finished uploading IDGNAZIM000133.tif\n",
      "Finished uploading IDGNAZIM000134.tif\n",
      "Finished uploading IDGNAZIM000135.tif\n",
      "Finished uploading IDGNAZIM000136.tif\n",
      "Finished uploading IDGNAZIM000137.tif\n",
      "Finished uploading IDGNAZIM000138.tif\n",
      "Finished uploading IDGNAZIM000139.tif\n",
      "Finished uploading IDGNAZIM00014.tif\n",
      "Finished uploading IDGNAZIM000140.tif\n",
      "Finished uploading IDGNAZIM000141.tif\n",
      "Finished uploading IDGNAZIM000142.tif\n",
      "Finished uploading IDGNAZIM000143.tif\n",
      "Finished uploading IDGNAZIM000144.tif\n",
      "Finished uploading IDGNAZIM000145.tif\n",
      "Finished uploading IDGNAZIM000146.tif\n",
      "Finished uploading IDGNAZIM000147.tif\n",
      "Finished uploading IDGNAZIM000148.tif\n",
      "Finished uploading IDGNAZIM000149.tif\n",
      "Finished uploading IDGNAZIM00015.tif\n",
      "Finished uploading IDGNAZIM000150.tif\n",
      "Finished uploading IDGNAZIM000151.tif\n",
      "Finished uploading IDGNAZIM000152.tif\n",
      "Finished uploading IDGNAZIM000153.tif\n",
      "Finished uploading IDGNAZIM000154.tif\n",
      "Finished uploading IDGNAZIM000155.tif\n",
      "Finished uploading IDGNAZIM000156.tif\n",
      "Finished uploading IDGNAZIM000157.tif\n",
      "Finished uploading IDGNAZIM000158.tif\n",
      "Finished uploading IDGNAZIM000159.tif\n",
      "Finished uploading IDGNAZIM00016.tif\n",
      "Finished uploading IDGNAZIM000160.tif\n",
      "Finished uploading IDGNAZIM000161.tif\n",
      "Finished uploading IDGNAZIM000162.tif\n",
      "Finished uploading IDGNAZIM000163.tif\n",
      "Finished uploading IDGNAZIM000164.tif\n",
      "Finished uploading IDGNAZIM000165.tif\n",
      "Finished uploading IDGNAZIM000166.tif\n",
      "Finished uploading IDGNAZIM000167.tif\n",
      "Finished uploading IDGNAZIM000168.tif\n",
      "Finished uploading IDGNAZIM000169.tif\n",
      "Finished uploading IDGNAZIM00017.tif\n",
      "Finished uploading IDGNAZIM000170.tif\n",
      "Finished uploading IDGNAZIM000171.tif\n",
      "Finished uploading IDGNAZIM000172.tif\n",
      "Finished uploading IDGNAZIM000173.tif\n",
      "Finished uploading IDGNAZIM000174.tif\n",
      "Finished uploading IDGNAZIM000175.tif\n",
      "Finished uploading IDGNAZIM000176.tif\n",
      "Finished uploading IDGNAZIM000177.tif\n",
      "Finished uploading IDGNAZIM000178.tif\n",
      "Finished uploading IDGNAZIM000179.tif\n",
      "Finished uploading IDGNAZIM00018.tif\n",
      "Finished uploading IDGNAZIM000180.tif\n",
      "Finished uploading IDGNAZIM000181.tif\n",
      "Finished uploading IDGNAZIM000182.tif\n",
      "Finished uploading IDGNAZIM000183.tif\n",
      "Finished uploading IDGNAZIM000184.tif\n",
      "Finished uploading IDGNAZIM000185.tif\n",
      "Finished uploading IDGNAZIM000186.tif\n",
      "Finished uploading IDGNAZIM000187.tif\n",
      "Finished uploading IDGNAZIM000188.tif\n",
      "Finished uploading IDGNAZIM000189.tif\n",
      "Finished uploading IDGNAZIM00019.tif\n",
      "Finished uploading IDGNAZIM000190.tif\n",
      "Finished uploading IDGNAZIM000191.tif\n",
      "Finished uploading IDGNAZIM000192.tif\n",
      "Finished uploading IDGNAZIM000193.tif\n",
      "Finished uploading IDGNAZIM000194.tif\n",
      "Finished uploading IDGNAZIM000195.tif\n",
      "Finished uploading IDGNAZIM000196.tif\n",
      "Finished uploading IDGNAZIM000197.tif\n",
      "Finished uploading IDGNAZIM000198.tif\n",
      "Finished uploading IDGNAZIM000199.tif\n",
      "Finished uploading IDGNAZIM0002.tif\n",
      "Finished uploading IDGNAZIM00020.tif\n",
      "Finished uploading IDGNAZIM000200.tif\n",
      "Finished uploading IDGNAZIM000201.tif\n",
      "Finished uploading IDGNAZIM000202.tif\n",
      "Finished uploading IDGNAZIM000203.tif\n",
      "Finished uploading IDGNAZIM000204.tif\n",
      "Finished uploading IDGNAZIM000205.tif\n",
      "Finished uploading IDGNAZIM000206.tif\n",
      "Finished uploading IDGNAZIM000207.tif\n",
      "Finished uploading IDGNAZIM000208.tif\n",
      "Finished uploading IDGNAZIM000209.tif\n",
      "Finished uploading IDGNAZIM00021.tif\n",
      "Finished uploading IDGNAZIM000210.tif\n",
      "Finished uploading IDGNAZIM000211.tif\n",
      "Finished uploading IDGNAZIM000212.tif\n",
      "Finished uploading IDGNAZIM000213.tif\n",
      "Finished uploading IDGNAZIM000214.tif\n",
      "Finished uploading IDGNAZIM000215.tif\n",
      "Finished uploading IDGNAZIM000216.tif\n",
      "Finished uploading IDGNAZIM000217.tif\n",
      "Finished uploading IDGNAZIM000218.tif\n",
      "Finished uploading IDGNAZIM000219.tif\n",
      "Finished uploading IDGNAZIM00022.tif\n",
      "Finished uploading IDGNAZIM000220.tif\n",
      "Finished uploading IDGNAZIM000221.tif\n",
      "Finished uploading IDGNAZIM000222.tif\n",
      "Finished uploading IDGNAZIM000223.tif\n",
      "Finished uploading IDGNAZIM000224.tif\n",
      "Finished uploading IDGNAZIM000225.tif\n",
      "Finished uploading IDGNAZIM000226.tif\n",
      "Finished uploading IDGNAZIM000227.tif\n",
      "Finished uploading IDGNAZIM000228.tif\n",
      "Finished uploading IDGNAZIM000229.tif\n",
      "Finished uploading IDGNAZIM00023.tif\n",
      "Finished uploading IDGNAZIM000230.tif\n",
      "Finished uploading IDGNAZIM000231.tif\n",
      "Finished uploading IDGNAZIM000232.tif\n",
      "Finished uploading IDGNAZIM000233.tif\n",
      "Finished uploading IDGNAZIM000234.tif\n",
      "Finished uploading IDGNAZIM000235.tif\n",
      "Finished uploading IDGNAZIM000236.tif\n",
      "Finished uploading IDGNAZIM000237.tif\n",
      "Finished uploading IDGNAZIM000238.tif\n",
      "Finished uploading IDGNAZIM000239.tif\n",
      "Finished uploading IDGNAZIM00024.tif\n",
      "Finished uploading IDGNAZIM000240.tif\n",
      "Finished uploading IDGNAZIM000241.tif\n",
      "Finished uploading IDGNAZIM000242.tif\n",
      "Finished uploading IDGNAZIM000243.tif\n",
      "Finished uploading IDGNAZIM000244.tif\n",
      "Finished uploading IDGNAZIM000245.tif\n",
      "Finished uploading IDGNAZIM000246.tif\n",
      "Finished uploading IDGNAZIM000247.tif\n",
      "Finished uploading IDGNAZIM000248.tif\n",
      "Finished uploading IDGNAZIM000249.tif\n",
      "Finished uploading IDGNAZIM00025.tif\n",
      "Finished uploading IDGNAZIM000250.tif\n",
      "Finished uploading IDGNAZIM00026.tif\n",
      "Finished uploading IDGNAZIM00027.tif\n",
      "Finished uploading IDGNAZIM00028.tif\n",
      "Finished uploading IDGNAZIM00029.tif\n",
      "Finished uploading IDGNAZIM0003.tif\n",
      "Finished uploading IDGNAZIM00030.tif\n",
      "Finished uploading IDGNAZIM00031.tif\n",
      "Finished uploading IDGNAZIM00032.tif\n",
      "Finished uploading IDGNAZIM00033.tif\n",
      "Finished uploading IDGNAZIM00034.tif\n",
      "Finished uploading IDGNAZIM00035.tif\n",
      "Finished uploading IDGNAZIM00036.tif\n",
      "Finished uploading IDGNAZIM00037.tif\n",
      "Finished uploading IDGNAZIM00038.tif\n",
      "Finished uploading IDGNAZIM00039.tif\n",
      "Finished uploading IDGNAZIM0004.tif\n",
      "Finished uploading IDGNAZIM00040.tif\n",
      "Finished uploading IDGNAZIM00041.tif\n",
      "Finished uploading IDGNAZIM00042.tif\n",
      "Finished uploading IDGNAZIM00043.tif\n",
      "Finished uploading IDGNAZIM00044.tif\n",
      "Finished uploading IDGNAZIM00045.tif\n",
      "Finished uploading IDGNAZIM00046.tif\n",
      "Finished uploading IDGNAZIM00047.tif\n",
      "Finished uploading IDGNAZIM00048.tif\n",
      "Finished uploading IDGNAZIM00049.tif\n",
      "Finished uploading IDGNAZIM0005.tif\n",
      "Finished uploading IDGNAZIM00050.tif\n",
      "Finished uploading IDGNAZIM00051.tif\n",
      "Finished uploading IDGNAZIM00052.tif\n",
      "Finished uploading IDGNAZIM00053.tif\n",
      "Finished uploading IDGNAZIM00054.tif\n",
      "Finished uploading IDGNAZIM00055.tif\n",
      "Finished uploading IDGNAZIM00056.tif\n",
      "Finished uploading IDGNAZIM00057.tif\n",
      "Finished uploading IDGNAZIM00058.tif\n",
      "Finished uploading IDGNAZIM00059.tif\n",
      "Finished uploading IDGNAZIM0006.tif\n",
      "Finished uploading IDGNAZIM00060.tif\n",
      "Finished uploading IDGNAZIM00061.tif\n",
      "Finished uploading IDGNAZIM00062.tif\n",
      "Finished uploading IDGNAZIM00063.tif\n",
      "Finished uploading IDGNAZIM00064.tif\n",
      "Finished uploading IDGNAZIM00065.tif\n",
      "Finished uploading IDGNAZIM00066.tif\n",
      "Finished uploading IDGNAZIM00067.tif\n",
      "Finished uploading IDGNAZIM00068.tif\n",
      "Finished uploading IDGNAZIM00069.tif\n",
      "Finished uploading IDGNAZIM0007.tif\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished uploading IDGNAZIM00070.tif\n",
      "Finished uploading IDGNAZIM00071.tif\n",
      "Finished uploading IDGNAZIM00072.tif\n",
      "Finished uploading IDGNAZIM00073.tif\n",
      "Finished uploading IDGNAZIM00074.tif\n",
      "Finished uploading IDGNAZIM00075.tif\n",
      "Finished uploading IDGNAZIM00076.tif\n",
      "Finished uploading IDGNAZIM00077.tif\n",
      "Finished uploading IDGNAZIM00078.tif\n",
      "Finished uploading IDGNAZIM00079.tif\n",
      "Finished uploading IDGNAZIM0008.tif\n",
      "Finished uploading IDGNAZIM00080.tif\n",
      "Finished uploading IDGNAZIM00081.tif\n",
      "Finished uploading IDGNAZIM00082.tif\n",
      "Finished uploading IDGNAZIM00083.tif\n",
      "Finished uploading IDGNAZIM00084.tif\n",
      "Finished uploading IDGNAZIM00085.tif\n",
      "Finished uploading IDGNAZIM00086.tif\n",
      "Finished uploading IDGNAZIM00087.tif\n",
      "Finished uploading IDGNAZIM00088.tif\n",
      "Finished uploading IDGNAZIM00089.tif\n",
      "Finished uploading IDGNAZIM0009.tif\n",
      "Finished uploading IDGNAZIM00090.tif\n",
      "Finished uploading IDGNAZIM00091.tif\n",
      "Finished uploading IDGNAZIM00092.tif\n",
      "Finished uploading IDGNAZIM00093.tif\n",
      "Finished uploading IDGNAZIM00094.tif\n",
      "Finished uploading IDGNAZIM00095.tif\n",
      "Finished uploading IDGNAZIM00096.tif\n",
      "Finished uploading IDGNAZIM00097.tif\n",
      "Finished uploading IDGNAZIM00098.tif\n",
      "Finished uploading IDGNAZIM00099.tif\n"
     ]
    }
   ],
   "source": [
    "# Upload images to an existing document\n",
    "#C:\\Users\\User\\PycharmProjects\\PersianChildhood\\MJLS_1-20\\MJLS_1-20\n",
    "IMAGE_FOLDER = \"training_data/updated_uri_gt_alto\"\n",
    "\n",
    "# ID of the document to which the images will be uploaded\n",
    "DOCUMENT_ID = 558  # Replace with your document ID\n",
    "\n",
    "@dataclass\n",
    "class ImageDataInfo:\n",
    "    filename: str\n",
    "    # Add other fields here if needed\n",
    "\n",
    "def upload_images_to_document(connector, folder_path, document_id):\n",
    "    \"\"\"Uploads image files to a specified document in eScriptorium.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.tif', '.png', '.jpg', '.jpeg')):  # Add/remove extensions as needed\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                # Prepare image data for upload\n",
    "                image_data = file.read()\n",
    "\n",
    "                # Create an instance of the dataclass\n",
    "                image_data_info = ImageDataInfo(filename=filename)\n",
    "\n",
    "                # Upload the image\n",
    "                #print(f\"Uploading {filename} to document {document_id}...\")\n",
    "                response = connector.create_document_part(\n",
    "                    document_pk=document_id, \n",
    "                    image_data_info=image_data_info, \n",
    "                    filename=filename, \n",
    "                    image_data=image_data\n",
    "                )\n",
    "                print(f\"Finished uploading {filename}\")\n",
    "\n",
    "                \n",
    "# Start the upload process\n",
    "upload_images_to_document(connector, IMAGE_FOLDER, DOCUMENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "433da79d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading IDGNAZIM0001.xml to document 558...\n"
     ]
    },
    {
     "ename": "RetryError",
     "evalue": "HTTPSConnectionPool(host='env-9828609.us.reclaim.cloud', port=443): Max retries exceeded with url: /api/documents/558/imports/ (Caused by ResponseError('too many 500 error responses'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:846\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    845\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url)\n\u001b[1;32m--> 846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:846\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    845\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url)\n\u001b[1;32m--> 846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "    \u001b[1;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 846 (2 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:846\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    845\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url)\n\u001b[1;32m--> 846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:836\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 836\u001b[0m     retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py:574\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    576\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='env-9828609.us.reclaim.cloud', port=443): Max retries exceeded with url: /api/documents/558/imports/ (Caused by ResponseError('too many 500 error responses'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m transcription_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreprocessed XML Import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Start the upload process\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mupload_xmls_to_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPAGE_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDOCUMENT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscription_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m, in \u001b[0;36mupload_xmls_to_document\u001b[1;34m(connector, folder_path, document_id, transcription_name)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Upload the image\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mconnector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_part_transcription\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_pk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtranscription_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtranscription_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#change to on if you want to override existing transciptions\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished uploading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\escriptorium_connector\\connector.py:730\u001b[0m, in \u001b[0;36mEscriptoriumConnector.upload_part_transcription\u001b[1;34m(self, document_pk, transcription_name, filename, file_data, override)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m override \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    728\u001b[0m     request_payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverride\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__post_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mdocuments/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdocument_pk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/imports/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupload_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\escriptorium_connector\\connector.py:306\u001b[0m, in \u001b[0;36mEscriptoriumConnector.__post_url\u001b[1;34m(self, url, payload, files, as_form_data)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_url\u001b[39m(\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    299\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m     as_form_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    303\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    304\u001b[0m     prepared_payload \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json\u001b[38;5;241m.\u001b[39mdumps(payload, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39mEnhancedJSONEncoder))\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_payload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m (\n\u001b[0;32m    309\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mpost(url, data\u001b[38;5;241m=\u001b[39mprepared_payload)\n\u001b[0;32m    310\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m as_form_data\n\u001b[0;32m    311\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mpost(url, json\u001b[38;5;241m=\u001b[39mprepared_payload)\n\u001b[0;32m    312\u001b[0m         )\n\u001b[0;32m    313\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:635\u001b[0m, in \u001b[0;36mSession.post\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\escriptorium_connector\\connector.py:111\u001b[0m, in \u001b[0;36mTimeoutHTTPAdapter.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:556\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _ProxyError):\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mRetryError\u001b[0m: HTTPSConnectionPool(host='env-9828609.us.reclaim.cloud', port=443): Max retries exceeded with url: /api/documents/558/imports/ (Caused by ResponseError('too many 500 error responses'))"
     ]
    }
   ],
   "source": [
    "# upload page XMLs for existing document and images\n",
    "#C:\\Users\\User\\PycharmProjects\\Arab Tribes\\Gazeetter_sample\\Gazeetter_sample_new1\\Gazeetter_sample_new1\n",
    "PAGE_FOLDER = \"training_data/fixed_uri_gt_alto\"\n",
    "\n",
    "# ID of the document to which the images will be uploaded\n",
    "DOCUMENT_ID = 558  # Replace with your document ID\n",
    "\n",
    "def upload_xmls_to_document(connector, folder_path, document_id, transcription_name):\n",
    "    \"\"\"Uploads image files to a specified document in eScriptorium.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.xml'):  # Add/remove extensions as needed\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                # Prepare image data for upload\n",
    "                file_data = file.read()\n",
    "\n",
    "                # Upload the image\n",
    "                print(f\"Uploading {filename} to document {document_id}...\")\n",
    "                response = connector.upload_part_transcription(\n",
    "                    document_pk=document_id, \n",
    "                    transcription_name = transcription_name, \n",
    "                    filename=filename, \n",
    "                    file_data=file_data,\n",
    "                    override='off' #change to on if you want to override existing transciptions\n",
    "                )\n",
    "                print(f\"Finished uploading {filename}\")\n",
    "\n",
    "                \n",
    "\n",
    "transcription_name = f'Preprocessed XML Import {datetime.date.today()}'\n",
    "# Start the upload process\n",
    "upload_xmls_to_document(connector, PAGE_FOLDER, DOCUMENT_ID, transcription_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549cf441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iterate_documents(root_folder, connector):\n",
    "    \"\"\"\n",
    "    Iterates through each subfolder in the root_folder. Each subfolder is considered a document.\n",
    "\n",
    "    :param root_folder: The path to the root folder containing document subfolders.\n",
    "    \"\"\"\n",
    "    # Iterate through each item in the root folder\n",
    "    for item in os.listdir(root_folder):\n",
    "        # Construct the full path of the item\n",
    "        item_path = os.path.join(root_folder, item)\n",
    "        \n",
    "        # Check if the item is a directory (i.e., a document folder)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"Document Name: {item}\")\n",
    "            main_script = \"Arabic\"\n",
    "            document_data = PostDocument(\n",
    "                name=item,\n",
    "                project=project_slug,\n",
    "                main_script=main_script,\n",
    "                read_direction=ReadDirection.RTL,\n",
    "                line_offset=LineOffset.BASELINE,\n",
    "                tags=[]\n",
    "            )\n",
    "            # Call the modified create_document method\n",
    "            document_pk = create_document_debug(connector, document_data)\n",
    "            \n",
    "            image_folder = item_path  # Assuming images are directly in the document folder\n",
    "            \n",
    "            upload_images_to_document(connector, image_folder, document_pk)\n",
    "            \n",
    "            transcription_name = 'transcript'\n",
    "            \n",
    "            page_folder = os.path.join(item_path, \"page\")  # The folder containing XML files\n",
    "            \n",
    "            upload_xmls_to_document(connector, page_folder, document_pk, transcription_name)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1eea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a44c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#playing with xml trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def preprocess_page_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for region in root.findall('.//{*}TextRegion'):\n",
    "        print(f\"TextRegion ID: {region.get('id')}\")\n",
    "        for line in region.findall('.//{*}TextLine'):\n",
    "            text = line.find('.//{*}Unicode').text\n",
    "            print(f\"  Line Text: {text}\")\n",
    "\n",
    "    for table in root.findall('.//{*}TableRegion'):\n",
    "        print(f\"TableRegion ID: {table.get('id')}\")\n",
    "        for cell in table.findall('.//{*}TableCell'):\n",
    "            print(f\"  TableCell ID: {cell.get('id')}\")\n",
    "            for line in cell.findall('.//{*}TextLine'):\n",
    "                text = line.find('.//{*}Unicode').text\n",
    "                print(f\"    Line Text: {text}\")\n",
    "\n",
    "preprocess_page_xml(f'{PAGE_FOLDER}/Gazeetter sample-0011.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def preprocess_page_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Ensure the xmlns attribute is set correctly\n",
    "    # This example uses a common PAGE XML namespace, adjust as needed\n",
    "    root.set(\"xmlns\", \"http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15\")\n",
    "\n",
    "    # Return the string representation of the modified XML\n",
    "    return ET.tostring(root, encoding='utf-8').decode('utf-8')\n",
    "\n",
    "def upload_xml_to_escriptorium(connector, xml_data, document_id, transcription_name):\n",
    "    # Prepare the payload for upload\n",
    "    files = {'upload_file': ('transcription.xml', xml_data)}\n",
    "    payload = {\n",
    "        'task': 'import-xml',\n",
    "        'name': transcription_name,\n",
    "        'override': 'on'  # or 'on' depending on your requirement\n",
    "    }\n",
    "\n",
    "    # Perform the upload\n",
    "    response = connector.upload_part_transcription(\n",
    "        document_pk=document_id, \n",
    "        transcription_name=transcription_name, \n",
    "        filename='transcription.xml', \n",
    "        file_data=xml_data\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "def upload_preprocessed_xml_to_document(connector, file_path, document_id, transcription_name):\n",
    "    # Preprocess the XML\n",
    "    preprocessed_xml = preprocess_page_xml(file_path)\n",
    "\n",
    "    # Upload the preprocessed XML to eScriptorium\n",
    "    response = upload_xml_to_escriptorium(connector, preprocessed_xml, document_id, transcription_name)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Upload successful.\")\n",
    "    else:\n",
    "        print(f\"Upload failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = f'{PAGE_FOLDER}/Gazeetter sample-0011.xml'  # Replace with your file path\n",
    "document_id = '3'  # Replace with your document ID\n",
    "transcription_name = 'your_transcription_name'  # Replace with your transcription name\n",
    "\n",
    "upload_preprocessed_xml_to_document(connector, file_path, document_id, transcription_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf22f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in os.listdir(PAGE_FOLDER)[-10:]:\n",
    "#     if filename.lower().endswith('.xml'):  # Add/remove extensions as needed\n",
    "#         file_path = os.path.join(PAGE_FOLDER, filename)\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             # Prepare image data for upload\n",
    "#             file_data = file.read()\n",
    "\n",
    "#         # Read the Page XML file\n",
    "# with open(file_path, 'rb') as file:\n",
    "#     file_data = file.read()\n",
    "\n",
    "# Upload the Page XML\n",
    "response = connector.upload_part_transcription(\n",
    "    document_pk=DOCUMENT_ID, \n",
    "    transcription_name=\"MyTranscription\", \n",
    "    filename=filename, \n",
    "    file_data=BytesIO(file_data),\n",
    "    override='on'  # Use 'on' if you want to override existing data\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
